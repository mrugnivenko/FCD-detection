{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "\n",
    "import utils.data_processor as data_processor\n",
    "imp.reload(data_processor)\n",
    "from utils.data_processor import *\n",
    "\n",
    "import utils.visualization_tools as visualization_tools\n",
    "imp.reload(visualization_tools)\n",
    "from utils.visualization_tools import *\n",
    "\n",
    "import utils.metrics as metrics\n",
    "imp.reload(metrics)\n",
    "from utils.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_3dmaps_to_point_cloud_and_labels(image_brain, image_hypo, size = 256, seg =None):\n",
    "    \"\"\" \n",
    "    Transforms 3d tensors of brain and hippocampus into pointcloud and labels for it. Both only coordinates and \n",
    "    coordinates + intensity modes are suppoted\n",
    "      Args:\n",
    "          image_brain: torch tensor of size [size,size,size] with 1 at the positions with brain and 0 otherwise\n",
    "          image_hypo: torch tensor of size [size,size,size] with 1 at the positions with hippocampus and 0 otherwise\n",
    "          size: size of the input tensors along each direction, default = 256\n",
    "          seg: torch tensor of size [size,size,size] with intensities of brain, default None\n",
    "      Output:\n",
    "          torch tensor of size [N, 3] is seg is None and [N, 4] otherwise and [N,] tensor with labels\n",
    "      \"\"\"\n",
    "    grid_x, grid_y, grid_z = torch.meshgrid(torch.tensor(range(size)),\\\n",
    "                                            torch.tensor(range(size)),\\\n",
    "                                            torch.tensor(range(size)))\n",
    "    if seg is None:\n",
    "        new = torch.cat((grid_x.unsqueeze(-1), grid_y.unsqueeze(-1),grid_z.unsqueeze(-1)), -1)\n",
    "    else:\n",
    "        new = torch.cat((grid_x.unsqueeze(-1).float(), \n",
    "                         grid_y.unsqueeze(-1).float(),\n",
    "                         grid_z.unsqueeze(-1).float(), \n",
    "                         seg.unsqueeze(-1).float()), -1)\n",
    "    pc_hypo = new[image_hypo==1,:]\n",
    "    pc_brain_without_hypo = new[(image_hypo==0)*(image_brain == 1),:]\n",
    "    return torch.cat([pc_hypo,pc_brain_without_hypo]),\\\n",
    "np.array([1] * pc_hypo.shape[0] + [0] * pc_brain_without_hypo.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_to_pc_and_labels(file, size = 256, target = [17,53], segfile = None):\n",
    "    \"\"\" \n",
    "    Procceses filename of brain into pointcloud with labels\n",
    "      Args:\n",
    "          file: filename \n",
    "          size: size of the input tensors along each direction is 256, but it can be maxpulled to size. Default = 256\n",
    "          segfile: file with segmentation path\n",
    "      Output:\n",
    "          torch tensor of size [N, 3] is seg is None and [N, 4] otherwise and [N,] tensor with labels\n",
    "      \"\"\"\n",
    "    \n",
    "    tmp = load_nii_to_array(file)\n",
    "    \n",
    "    hypo = tmp.copy()\n",
    "    hypo[~np.isin(hypo,target)]=0\n",
    "    hypo[np.isin(hypo,target)]=1\n",
    "    \n",
    "    brain = tmp.copy()\n",
    "    brain[~np.isin(brain,[0])]=1\n",
    "    \n",
    "    if segfile is not None:\n",
    "        seg = load_nii_to_array(segfile)\n",
    "        seg = brain * seg\n",
    "    else:\n",
    "        seg = None\n",
    "    \n",
    "    if size == 32:\n",
    "        m = torch.nn.MaxPool3d(8, 8)\n",
    "        brain = m(torch.tensor(brain, dtype=torch.float64).unsqueeze(0).unsqueeze(0)).detach().squeeze()\n",
    "        hypo = m(torch.tensor(hypo, dtype=torch.float64).unsqueeze(0).unsqueeze(0)).detach().squeeze()\n",
    "        if segfile is not None:\n",
    "            seg = m(torch.tensor(seg, dtype=torch.float64).unsqueeze(0).unsqueeze(0)).detach().squeeze()\n",
    "    \n",
    "    pc,labels = binary_3dmaps_to_point_cloud_and_labels(brain, hypo, size = size, seg = seg)\n",
    "    \n",
    "    return pc, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_norm(pc):\n",
    "    \"\"\" \n",
    "    Normalises given point cloud\n",
    "      Args:\n",
    "          pc: pointcloud - torchtensor of size [N, 3] or [N, 4]\n",
    "      Output:\n",
    "          normalised pointcloud of same shape\n",
    "    \"\"\"\n",
    "    \n",
    "    pmin = np.min(pc[:,:3])\n",
    "    pmax = np.max(pc[:,:3])\n",
    "    pc -= (pmin + pmax) / 2\n",
    "    scale = np.max(np.linalg.norm(pc[:,:3], axis=1))\n",
    "    pc[:,:3] *= 1.0 / scale\n",
    "    \n",
    "    if pc.shape[1] == 4:\n",
    "        pc[:,3:4] -= np.mean(pc[:,3:4])\n",
    "        pc[:,3:4] /= np.max(abs(pc[:,3:4]))\n",
    "\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for experiment 1 creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 32\n",
    "TARGET_LABELS = [17,53]\n",
    "TEST_DATA = 100\n",
    "POSTFIX = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs, labels = [], []\n",
    "for file in tqdm(glob.glob('../datasets/fcd_classification_bank/*_aparc+aseg.nii*')):\n",
    "    pc,label = filename_to_pc_and_labels(file, \n",
    "                                         size = SIZE,\n",
    "                                         target = TARGET_LABELS\n",
    "                                        )\n",
    "    pc = pc_norm(np.array(pc.detach(),dtype = float))\n",
    "    pcs.append(pc)\n",
    "    labels.append(label)\n",
    "    \n",
    "sc_labels = [np.array(0)]*len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_test,labels_test, sc_labels_test = pcs[:TEST_DATA],labels[:TEST_DATA],sc_labels[:TEST_DATA]\n",
    "pcs_train,labels_train, sc_labels_train = pcs[TEST_DATA:],labels[TEST_DATA:],sc_labels[TEST_DATA:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = (pcs_test,labels_test, sc_labels_test)\n",
    "data_train = (pcs_train,labels_train, sc_labels_train)\n",
    "with open(f'../CloserLook3D/pytorch/data/BrainData/test_data{POSTFIX}.pkl', 'wb') as f:\n",
    "    pickle.dump(data_test, f)\n",
    "with open(f'../CloserLook3D/pytorch/data/BrainData/trainval_data{POSTFIX}.pkl', 'wb') as f:\n",
    "    pickle.dump(data_train, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for experiment 2 creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 32\n",
    "TARGET_LABELS = [17,53]\n",
    "TEST_DATA = 100\n",
    "POSTFIX = '_2exp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs, labels = [], []\n",
    "for file in tqdm(glob.glob('../datasets/fcd_classification_bank/*_aparc+aseg.nii*')):\n",
    "    try:\n",
    "        segfile = [x for x in glob.glob(file.split('aparc+aseg.nii')[0]+'*') if 'aparc+aseg' not in x][0]\n",
    "    except Exception:\n",
    "        pass\n",
    "    pc,label = filename_to_pc_and_labels(file, \n",
    "                                         size = SIZE,\n",
    "                                         target = TARGET_LABELS, \n",
    "                                         segfile = seg_file\n",
    "                                        )\n",
    "    pc = pc_norm(np.array(pc.detach(),dtype = float))\n",
    "    pcs.append(pc)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_labels = [np.array(0)]*len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_test,labels_test, sc_labels_test = pcs[:TEST_DATA],labels[:TEST_DATA],sc_labels[:TEST_DATA]\n",
    "pcs_train,labels_train, sc_labels_train = pcs[TEST_DATA:],labels[TEST_DATA:],sc_labels[TEST_DATA:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = (pcs_test,labels_test, sc_labels_test)\n",
    "data_train = (pcs_train,labels_train, sc_labels_train)\n",
    "with open(f'../CloserLook3D/pytorch/data/BrainData/test_data{POSTFIX}.pkl', 'wb') as f:\n",
    "    pickle.dump(data_test, f)\n",
    "with open(f'../CloserLook3D/pytorch/data/BrainData/trainval_data{POSTFIX}.pkl', 'wb') as f:\n",
    "    pickle.dump(data_train, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional functions for PC generation for FCD task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcd_filename_to_pc_and_labels(file, file_mask, \n",
    "                                  size = 256, \n",
    "                                  LIST_FCD = [ 8, 10, 11, 12, 13, 16, 17, 18, 26, 47, 49, 50,\n",
    "                                              51, 52, 53, 54, 58, 85, 251, 252, 253, 254, 255], \n",
    "                                  segfile = None):\n",
    "    \"\"\" \n",
    "    Procceses filename of brain and mask into pointcloud with labels\n",
    "      Args:\n",
    "          file: path to brain file\n",
    "          file_mask: path to mask file\n",
    "          size: size of the input tensors along each direction is 256, but it can be maxpulled to size. Default = 256\n",
    "          segfile: file with segmentation path\n",
    "      Output:\n",
    "          torch tensor of size [N, 3] is seg is None and [N, 4] otherwise and [N,] tensor with labels\n",
    "      \"\"\"\n",
    "    \n",
    "    brain = load_nii_to_array(file)\n",
    "    brain[0][0][(np.isin(brain[0][0], LIST_FCD))] = 1.0\n",
    "    brain[brain >= 1000] = 1.0\n",
    "    brain[brain != 1] = 0.0\n",
    "    \n",
    "    \n",
    "    mask = load_nii_to_array(file_mask)\n",
    "    \n",
    "    if segfile is not None:\n",
    "        seg = load_nii_to_array(segfile)\n",
    "        seg = torch.tensor(brain * seg, dtype=torch.float64).detach()\n",
    "    else:\n",
    "        seg = None\n",
    "    \n",
    "    if size == 32:\n",
    "        m = torch.nn.MaxPool3d(8, 8)\n",
    "        brain = m(torch.tensor(brain, dtype=torch.float64).unsqueeze(0).unsqueeze(0)).detach().squeeze()\n",
    "        mask = m(torch.tensor(hypo, dtype=torch.float64).unsqueeze(0).unsqueeze(0)).detach().squeeze()\n",
    "        if segfile is not None:\n",
    "            seg = m(torch.tensor(seg, dtype=torch.float64).unsqueeze(0).unsqueeze(0)).detach().squeeze()\n",
    "    \n",
    "    pc,labels = binary_3dmaps_to_point_cloud_and_labels(brain, mask, size = size, seg = seg)\n",
    "    \n",
    "    return pc, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for experiment 3 creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256\n",
    "UPSAMPLE_RATE = 10\n",
    "POSTFIX = '_3exp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:29<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "pcs, labels = [], []\n",
    "for file in tqdm(glob.glob('../datasets/fcd_classification_bank/fcd_*_aparc+aseg.nii*')):\n",
    "    peace = file.split('/')[-1].split('_aparc')[0]\n",
    "    try:\n",
    "        file_mask = glob.glob(f'../masks/{peace}*')[0]\n",
    "    except Exception:\n",
    "        pass\n",
    "    pc, label = fcd_filename_to_pc_and_labels(file, file_mask, \n",
    "                                  size = SIZE)\n",
    "    pc = pc_norm(np.array(pc.detach(),dtype = float))\n",
    "    pcs.append(pc)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_labels = [np.array(0)]*len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(len(pcs)):\n",
    "    TEST_DATA_INDEXES = [e]\n",
    "    pcs_test,labels_test, sc_labels_test = [pcs[i] for i in TEST_DATA_INDEXES]*UPSAMPLE_RATE,\\\n",
    "                                            [labels[i] for i in TEST_DATA_INDEXES]*UPSAMPLE_RATE,\\\n",
    "                                            [sc_labels[i] for i in TEST_DATA_INDEXES]*UPSAMPLE_RATE\n",
    "    pcs_train,labels_train, sc_labels_train = [pcs[i] for i in range(len(pcs)) if i not in TEST_DATA_INDEXES]*UPSAMPLE_RATE,\\\n",
    "                                            [labels[i] for i in range(len(pcs)) if i not in TEST_DATA_INDEXES]*UPSAMPLE_RATE,\\\n",
    "                                            [sc_labels[i] for i in range(len(pcs)) if i not in TEST_DATA_INDEXES]*UPSAMPLE_RATE\n",
    "    \n",
    "    data_test = (pcs_test,labels_test, sc_labels_test)\n",
    "    data_train = (pcs_train,labels_train, sc_labels_train)\n",
    "    with open(f'../CloserLook3D/pytorch/data/BrainData/test_data{POSTFIX}_{e}.pkl', 'wb') as f:\n",
    "        pickle.dump(data_test, f)\n",
    "    with open(f'../CloserLook3D/pytorch/data/BrainData/trainval_data{POSTFIX}_{e}.pkl', 'wb') as f:\n",
    "        pickle.dump(data_train, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for experiment 4 creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256\n",
    "UPSAMPLE_RATE = 10\n",
    "POSTFIX = '_4exp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:43<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "pcs, labels = [], []\n",
    "for file in tqdm(glob.glob('../datasets/fcd_classification_bank/fcd_*_aparc+aseg.nii*')):\n",
    "    peace = file.split('/')[-1].split('_aparc')[0]\n",
    "    try:\n",
    "        file_mask = glob.glob(f'../masks/{peace}*')[0]\n",
    "    except Exception:\n",
    "        pass\n",
    "    segfile = [x for x in glob.glob(file.split('aparc+aseg.nii')[0]+'*') if 'aparc+aseg' not in x][0]\n",
    "    pc, label = fcd_filename_to_pc_and_labels(file, file_mask, \n",
    "                                  size = 256, segfile = segfile)\n",
    "    pc = pc_norm(np.array(pc.detach(),dtype = float))\n",
    "    pcs.append(pc)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_labels = [np.array(0)]*len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(len(pcs)):\n",
    "    TEST_DATA_INDEXES = [e]\n",
    "    pcs_test,labels_test, sc_labels_test = [pcs[i] for i in TEST_DATA_INDEXES]*UPSAMPLE_RATE,\\\n",
    "                                            [labels[i] for i in TEST_DATA_INDEXES]*UPSAMPLE_RATE,\\\n",
    "                                            [sc_labels[i] for i in TEST_DATA_INDEXES]*UPSAMPLE_RATE\n",
    "    pcs_train,labels_train, sc_labels_train = [pcs[i] for i in range(len(pcs)) if i not in TEST_DATA_INDEXES]*UPSAMPLE_RATE,\\\n",
    "                                            [labels[i] for i in range(len(pcs)) if i not in TEST_DATA_INDEXES]*UPSAMPLE_RATE,\\\n",
    "                                            [sc_labels[i] for i in range(len(pcs)) if i not in TEST_DATA_INDEXES]*UPSAMPLE_RATE\n",
    "    \n",
    "    data_test = (pcs_test,labels_test, sc_labels_test)\n",
    "    data_train = (pcs_train,labels_train, sc_labels_train)\n",
    "    with open(f'../CloserLook3D/pytorch/data/BrainData/test_data{POSTFIX}_{e}.pkl', 'wb') as f:\n",
    "        pickle.dump(data_test, f)\n",
    "    with open(f'../CloserLook3D/pytorch/data/BrainData/trainval_data{POSTFIX}_{e}.pkl', 'wb') as f:\n",
    "        pickle.dump(data_train, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
